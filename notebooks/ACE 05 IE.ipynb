{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import json\n",
    "from io import open\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set_color_codes()\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data and convert it to the NER task format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.loads(open('../data/ace_05_yaat.json', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "class Vocab():\n",
    "    \"\"\" A convenience vocabulary wrapper \n",
    "    \n",
    "    TODO: Add in sampling table functionality\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 tokens=None, \n",
    "                 min_count=5,\n",
    "                 pad_token='<PAD>', \n",
    "                 unk_token='<UNK>'):\n",
    "        self.min_count=min_count\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.use(tokens)\n",
    "#         self.make_sampling_table()\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        \"\"\" The total number of tokens seen by the vocabulary.\n",
    "        \n",
    "        This **does not** include tokens which have not been seen `min_count` times\n",
    "        \"\"\"\n",
    "        return self._n\n",
    "\n",
    "    @property\n",
    "    def v(self):\n",
    "        \"\"\" The total number of unique tokens in the vocabulary.\n",
    "        \n",
    "        This **does not** include tokens which have not been seen `min_count` times\n",
    "        \"\"\"\n",
    "        return self._v\n",
    "\n",
    "    @property\n",
    "    def pad(self):\n",
    "        \"\"\" Return the PAD token \"\"\"\n",
    "        return self.pad_token\n",
    "\n",
    "    @property\n",
    "    def ipad(self):\n",
    "        \"\"\" Return the index of the PAD token \"\"\"\n",
    "        return self.idx(self.pad_token)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        \"\"\" Return the UNK token \"\"\"\n",
    "        return self.unk_token\n",
    "\n",
    "    @property\n",
    "    def iunk(self):\n",
    "        \"\"\" Return the index of the UNK token \"\"\"\n",
    "        return self.idx(self.unk_token)\n",
    "\n",
    "    def idx(self, token):\n",
    "        \"\"\" Return the index of a token or the index of UNK if not in vocab.\n",
    "        \n",
    "        Additionally this will return UNK if the token is in the vocab \n",
    "        but has yet to be seen `min_count` times\n",
    "        \"\"\"\n",
    "        if token == self.pad_token:\n",
    "            return self._vocab2idx[token]\n",
    "        elif token in self.vocabset:\n",
    "            if self.count_index[token] >= self.min_count:\n",
    "                return self._vocab2idx[token]\n",
    "            else:\n",
    "                return self._vocab2idx[self.unk_token]\n",
    "        else:\n",
    "            return self._vocab2idx[self.unk_token]\n",
    "\n",
    "    def token(self, idx):\n",
    "        \"\"\" Return the token corresponding to the input index `idx`.\n",
    "        \n",
    "        If the index is not in the vocabulary, or it is the index\n",
    "        of a token that has not been seen `min_count` times,\n",
    "        the UNK token is returned instead\n",
    "        \"\"\"\n",
    "        if idx in self.idxset:\n",
    "            token = self._idx2vocab[idx]\n",
    "            if self.count_index[token] >= self.min_count:\n",
    "                return token\n",
    "            else:\n",
    "                return self._idx2vocab[self.unk_token]\n",
    "        else:\n",
    "            return self._idx2vocab[self.unk_token]\n",
    "\n",
    "    def use(self, tokens):\n",
    "        \"\"\" Create the vocabulary, using these tokens.\n",
    "        \n",
    "        This method will reset the vocab with these tokens.\n",
    "        \n",
    "        `tokens` is expected to be a flat list.\n",
    "        \"\"\"\n",
    "        self.count_index = Counter()\n",
    "        self._vocab2idx = {self.pad_token:0,\n",
    "                           self.unk_token:1}\n",
    "        self._idx2vocab = {0:self.pad_token,\n",
    "                           1:self.unk_token}\n",
    "        if tokens:\n",
    "            self.add(tokens)\n",
    "\n",
    "    def add(self, tokens):\n",
    "        \"\"\" Add these tokens to the vocabulary.\n",
    "        \n",
    "        This can be used iteratively, adding, say, one sentence at a time.\n",
    "        \n",
    "        NOTE: Expects `tokens` to be a flat list.\n",
    "        \"\"\"\n",
    "        # increment counts of tokens seen here\n",
    "        for token in tokens:\n",
    "            self.count_index[token] += 1\n",
    "        \n",
    "        # add tokens to the vocabulary if they are new\n",
    "        token_set = set(tokens)\n",
    "        for token in token_set:\n",
    "            if token not in self._vocab2idx:\n",
    "                new_idx = len(self._vocab2idx)\n",
    "                self._vocab2idx[token] = new_idx\n",
    "                self._idx2vocab[new_idx] = token\n",
    "        \n",
    "        # now precompute commonly used properties of the vocab (ignoring infrequent tokens)\n",
    "        self.vocabset = set([ token for (token, count) in self.count_index.most_common()\n",
    "                              if count >= self.min_count ]) \n",
    "        self.vocabset |= set([self.pad_token, self.unk_token])\n",
    "        self.idxset = set([ self._vocab2idx[token] for token in self.vocabset ])\n",
    "        self._n = sum( count for count in self.count_index.values() if count >= self.min_count )\n",
    "        self._v = sum( 1 for count in self.count_index.values() if count >= self.min_count ) + 2 # <PAD> and <UNK>\n",
    "        \n",
    "    def drop_infrequent(self):\n",
    "        \"\"\" Drop all words from the vocabulary that have not been seen `min_count` times\n",
    "        and recompute the vocabulary indices.\n",
    "        \n",
    "        This is useful for when the vocab has a long tail of infrequent words\n",
    "        that we no longer wish to account for.\n",
    "        \n",
    "        NOTE: This changes indices of tokens in the vocab!\n",
    "        \"\"\"\n",
    "        # remove all infrequent tokens from the count index\n",
    "        to_remove = set(self.count_index.keys()) - self.vocabset\n",
    "        for token in to_remove:\n",
    "            self.count_index.pop(token)\n",
    "            \n",
    "        # now reset the vocab dicts and reindex the tokens\n",
    "        self._vocab2idx = {self.pad_token:0,\n",
    "                           self.unk_token:1}\n",
    "        self._idx2vocab = {0:self.pad_token,\n",
    "                           1:self.unk_token}\n",
    "        for token in self.count_index.keys():\n",
    "            new_idx = len(self._vocab2idx)\n",
    "            self._vocab2idx[token] = new_idx\n",
    "            self._idx2vocab[new_idx] = token\n",
    "            \n",
    "        # precompute commonly used properties of the vocab\n",
    "        self.vocabset = set(self._vocab2idx.keys())\n",
    "        self.idxset = set(self._idx2vocab.keys())\n",
    "        self._n = sum( count for count in self.count_index.values() )\n",
    "        self._v = len(self._vocab2idx)\n",
    "\n",
    "    def count(self, token):\n",
    "        \"\"\" Get the count of a token.  \n",
    "        \n",
    "        This includes tokens with countes below `min_count`,\n",
    "        which have been seen but are not included in the vocab.\n",
    "        \"\"\"\n",
    "        return self.count_index[token]\n",
    "\n",
    "#     def make_sampling_table(self, power_scalar=.75):\n",
    "#         # from 0 to V-1, get the frequency\n",
    "#         self.vocab_distribution = np.array([ (self.count_index[self._idx2vocab[idx]]/float(self._n))**power_scalar\n",
    "#                                     for idx in range(len(self.idxset))])\n",
    "#         self.vocab_distribution /= np.sum(self.vocab_distribution).astype(np.float)\n",
    "\n",
    "#     def sample(self, sample_shape):\n",
    "#         # sample a tensor of indices\n",
    "#         # by walking up the CDF\n",
    "#         # setting each position to the index\n",
    "#         # of the word which is the closest\n",
    "#         # word with that CDF\n",
    "#         sums = np.zeros(sample_shape)\n",
    "#         rands = npr.uniform(size=sample_shape)\n",
    "#         idxs = np.zeros(sample_shape)\n",
    "#         for i in range(len(self.vocab_distribution)):\n",
    "#             sums += self.vocab_distribution[i]\n",
    "#             idxs[sums <= rands] = i\n",
    "#         return idxs.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348986 total tokens and 20192 types\n",
      "324491 total tokens and 4826 types with mincount > 5\n"
     ]
    }
   ],
   "source": [
    "token_vocab = Vocab(min_count=0)\n",
    "for doc in data.values():\n",
    "    token_vocab.add(doc['tokens'])\n",
    "print '{} total tokens and {} types'.format(token_vocab.n, token_vocab.v)\n",
    "\n",
    "token_vocab = Vocab(min_count=5)\n",
    "for doc in data.values():\n",
    "    token_vocab.add(doc['tokens'])\n",
    "print '{} total tokens and {} types with mincount > 5'.format(token_vocab.n, token_vocab.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'event-anchor', u'value', u'entity'])\n"
     ]
    }
   ],
   "source": [
    "mention_types = set([ annotation['node-type'] for doc in data.values() for annotation in doc['annotations']\n",
    "                      if annotation['ann-type'] == 'node'])\n",
    "print mention_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for doc in data.values()[:1]:\n",
    "#     for annotation in doc['annotations']:\n",
    "#         if annotation['ann-type'] == 'node':\n",
    "#             print annotation['ann-span']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Entity_BIO_map(mention_labels, annotation):\n",
    "    \"\"\" Uses BIO scheme (untyped) for entities only \"\"\"\n",
    "    if annotation['node-type'] == 'entity':\n",
    "        left, right = tuple(annotation['ann-span'])\n",
    "        mention_labels[left] = 'B'\n",
    "        for i in range(1, right-left+1):\n",
    "            mention_labels[left+i] = 'I'\n",
    "    return mention_labels\n",
    "\n",
    "def Entity_typed_BIO_map(mention_labels, annotation):\n",
    "    \"\"\" Uses BIO scheme (typed) for entities only \"\"\"\n",
    "    if annotation['node-type'] == 'entity':\n",
    "        mention_type = annotation['type']\n",
    "        left, right = tuple(annotation['ann-span'])\n",
    "        mention_labels[left] = 'B-'+mention_type\n",
    "        for i in range(1, right-left+1):\n",
    "            mention_labels[left+i] = 'I-'+mention_type\n",
    "    return mention_labels\n",
    "\n",
    "def Entity_BILOU_map(mention_labels, annotation):\n",
    "    \"\"\" Uses BILOU scheme (untyped) for entities only \"\"\"\n",
    "    if annotation['node-type'] == 'entity':\n",
    "        left, right = tuple(annotation['ann-span'])\n",
    "        if left == right:\n",
    "            mention_labels[left] = 'U'\n",
    "        else:\n",
    "            mention_labels[left] = 'B'\n",
    "            for i in range(1, right-left):\n",
    "                mention_labels[left+i] = 'I'\n",
    "            mention_labels[right] = 'L'\n",
    "    return mention_labels\n",
    "\n",
    "def Entity_typed_BILOU_map(mention_labels, annotation):\n",
    "    \"\"\" Uses BILOU scheme (typed) for entities only \"\"\"\n",
    "    if annotation['node-type'] == 'entity':\n",
    "        mention_type = annotation['type']\n",
    "        left, right = tuple(annotation['ann-span'])\n",
    "        if left == right:\n",
    "            mention_labels[left] = 'U-'+mention_type\n",
    "        else:\n",
    "            mention_labels[left] = 'B-'+mention_type\n",
    "            for i in range(1, right-left):\n",
    "                mention_labels[left+i] = 'I-'+mention_type\n",
    "            mention_labels[right] = 'L-'+mention_type\n",
    "    return mention_labels\n",
    "\n",
    "def compute_flat_mention_labels(doc, scheme_func=Entity_BIO_map):\n",
    "    \"\"\" Takes a YAAT style document and computes token-level mention label list.\n",
    "    \n",
    "    This function only considers the outermost spans (as per ACE evaluation)\n",
    "    by editing the mentions of shortest span-length to longest.\n",
    "    Thus wider mentions will override narrower nested mentions.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    tokens: ['part', 'of' , 'mention', '.', 'not', 'part']\n",
    "    with annotation: {'ann-type':'node',\n",
    "                      'node-type':'entity',\n",
    "                      'ann-span':[0,2]}\n",
    "    with scheme: BIO\n",
    "    \n",
    "    yields:\n",
    "    mention_labels = ['B', 'I', 'I', 'O', 'O', 'O']\n",
    "    \"\"\"\n",
    "    mention_labels = ['O' for token in doc['tokens']]\n",
    "    mentions = [ annotation for annotation in doc['annotations'] if annotation['ann-type'] == 'node' ]\n",
    "    for annotation in sorted(mentions, key=lambda x:x['ann-span'][1]-x['ann-span'][0]):\n",
    "        mention_labels = scheme_func(mention_labels, annotation)\n",
    "    return mention_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc['boundary_labels'] = compute_flat_mention_labels(doc, Entity_BILOU_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for token, label in zip(doc['tokens'], doc['mention_labels']):\n",
    "#     print token, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boundary_vocab = Vocab(min_count=0)\n",
    "for doc in data.values():\n",
    "    doc['boundary_labels'] = compute_flat_mention_labels(doc, Entity_BIO_map)\n",
    "    boundary_vocab.add(doc['boundary_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374 train documents and 161 test documents\n"
     ]
    }
   ],
   "source": [
    "xy = [(doc['tokens'], doc['boundary_labels']) for doc in data.values()]\n",
    "npr.shuffle(xy)\n",
    "\n",
    "test_split = int(len(xy)*.7)\n",
    "xy_train, xy_test = xy[:test_split], xy[test_split:]\n",
    "\n",
    "x_train = [d[0] for d in xy_train]\n",
    "y_train = [d[1] for d in xy_train]\n",
    "x_test = [d[0] for d in xy_test]\n",
    "y_test = [d[1] for d in xy_test]\n",
    "\n",
    "print '{} train documents and {} test documents'.format(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before we do conversions, we need to drop unfrequent words from the vocab and reindex it\n",
    "token_vocab.drop_infrequent()\n",
    "boundary_vocab.drop_infrequent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    \"\"\" Generates batches of the data\"\"\"\n",
    "    def __init__(self, x, y, batch_size=64):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        if len(self.x) % self.batch_size:\n",
    "            # batch size doesn't divide the data\n",
    "            # so we will have a remainder (smaller) batch\n",
    "            self.n_batches = len(self.x) // self.batch_size + 1 \n",
    "        else:\n",
    "            self.n_batches = len(self.x) // self.batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        xy = zip(self.x, self.y)\n",
    "        npr.shuffle(xy)\n",
    "        \n",
    "        # chunk it into batches\n",
    "        batches = [ xy[i*self.batch_size:(i+1)*self.batch_size] \n",
    "                   for i in range(len(xy)//self.batch_size)]\n",
    "        \n",
    "        # iterate over the batches, unpacking into x,y\n",
    "        for batch in batches:\n",
    "            yield zip(*batch)\n",
    "            \n",
    "class BucketedBatchGenerator():\n",
    "    \"\"\" Generates batches of that have approximately homogenous sequence lengths w/in batch\"\"\"\n",
    "    def __init__(self, x, y, batch_size=64):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        if len(self.x) % self.batch_size:\n",
    "            # batch size doesn't divide the data\n",
    "            # so we will have a remainder (smaller) batch\n",
    "            self.n_batches = len(self.x) // self.batch_size + 1 \n",
    "        else:\n",
    "            self.n_batches = len(self.x) // self.batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # sort data from shortest to longest\n",
    "        xy = sorted(zip(self.x, self.y), key=lambda x:len(x[0]))\n",
    "        # now chunk it into batches\n",
    "        batches = [ xy[i*self.batch_size:(i+1)*self.batch_size] \n",
    "                   for i in range(len(xy)//self.batch_size)]\n",
    "        # shuffle the batches\n",
    "        npr.shuffle(batches)\n",
    "        \n",
    "        # iterate over the batches, shuffling and unpacking into x,y\n",
    "        for batch in batches:\n",
    "            npr.shuffle(batch)\n",
    "            yield zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Model():\n",
    "    def __init__(self, session, model_name, options):\n",
    "        self.session = session\n",
    "        self.model_name = model_name\n",
    "        self.token_embed_size = options['token_embed_size']\n",
    "        self.rnn_hidden_size = options['rnn_hidden_size']\n",
    "        self.learning_rate = options['learning_rate']\n",
    "        self.token_vocab = options['token_vocab']\n",
    "        self.boundary_vocab = options['boundary_vocab']\n",
    "        \n",
    "        if 'checkpoint_dir' in options:\n",
    "            self.model_name = os.path.join(options['checkpoint_dir'], self.model_name)\n",
    "        \n",
    "        self.histories = {}\n",
    "    \n",
    "        print \"Building graph...\",\n",
    "        self.manipulate_options()\n",
    "        self.global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.best_global_step_tensor = tf.Variable(0, trainable=False, name='best_global_step')\n",
    "        self.best_valid_loss_tensor = tf.Variable(1e50, trainable=False, name='best_valid_loss')\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.build_forward()\n",
    "        self.build_loss()\n",
    "        self.build_optimizer()\n",
    "        \n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        print \"Done\"\n",
    "        \n",
    "    @property\n",
    "    def global_step(self):\n",
    "        return self.global_step_tensor.eval()\n",
    "    \n",
    "    @property\n",
    "    def best_global_step(self):\n",
    "        return self.best_global_step_tensor.eval()\n",
    "    \n",
    "    @best_global_step.setter\n",
    "    def best_valid_global_step(self, val):\n",
    "        self.session.run(self.best_global_step_tensor.assign(val))\n",
    "    \n",
    "    @property\n",
    "    def best_valid_loss(self):\n",
    "        return self.best_valid_loss_tensor.eval()\n",
    "    \n",
    "    @best_valid_loss.setter\n",
    "    def best_valid_loss(self, val):\n",
    "        self.session.run(self.best_valid_loss_tensor.assign(val))\n",
    "        \n",
    "    def manipulate_options(self):\n",
    "        \"\"\" For manipulating options before building the graph. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def build_forward(self):\n",
    "        # inputs\n",
    "        self.token_seqs = tf.placeholder(dtype=tf.int32, \n",
    "                                         shape=[None, None])\n",
    "        self.seq_lens = tf.placeholder(dtype=tf.int32,\n",
    "                                      shape=[None])\n",
    "        self.batch_size = tf.shape(self.token_seqs)[0]\n",
    "        self.seq_len = tf.shape(self.token_seqs)[1]\n",
    "        \n",
    "        # initializers\n",
    "        rand_init = tf.random_normal_initializer(mean=0.0, \n",
    "                                                 stddev=1.0, \n",
    "                                                 seed=42, \n",
    "                                                 dtype=tf.float32)\n",
    "        ones_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "        \n",
    "        # embed the sequence tokens\n",
    "        embeddings = tf.get_variable(\"token_embeddings\",\n",
    "                                     shape=[self.token_vocab.v, self.token_embed_size],\n",
    "                                     initializer=rand_init)\n",
    "        embedded_seqs = tf.nn.embedding_lookup(embeddings, self.token_seqs)\n",
    "        \n",
    "        # process sequence with rnn\n",
    "        cell = tf.nn.rnn_cell.GRUCell(self.rnn_hidden_size)\n",
    "        initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, embedded_seqs, \n",
    "                                           sequence_length=self.seq_lens, \n",
    "                                           initial_state=initial_state)\n",
    "        \n",
    "        # make predictions\n",
    "        W_o = tf.get_variable(\"W_out\", \n",
    "                              shape=[self.rnn_hidden_size, self.boundary_vocab.v],\n",
    "                              initializer=rand_init)\n",
    "        b_o = tf.get_variable(\"b_out\", \n",
    "                              shape=[self.boundary_vocab.v],\n",
    "                              initializer=ones_init)\n",
    "        \n",
    "        outputs_flat_across_time = tf.reshape(outputs, [self.batch_size*self.seq_len, -1])\n",
    "        logits = tf.matmul(outputs_flat_across_time, W_o) + b_o\n",
    "        self.logits = tf.reshape(logits, [self.batch_size, self.seq_len, -1])\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        self.preds = tf.argmax(self.logits, dimension=2)\n",
    "        \n",
    "    def build_loss(self):\n",
    "        def sparse_xent((logits, labels)):\n",
    "            \"\"\" Returns the cross entropy of a single timestep across the batch.\n",
    "            \n",
    "            Allows for us to use map to get the loss of a batch w/ unknown\n",
    "            sequence lengths and batch size (no need for static unrolling of loss)\n",
    "            \"\"\"\n",
    "            return tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        self.seq_labels = tf.placeholder(dtype=tf.int32, \n",
    "                                         shape=[None, None])\n",
    "        self.seq_label_weights = tf.placeholder(dtype=tf.float32,\n",
    "                                                shape=[None, None])\n",
    "        # convert everything to time-major\n",
    "        logits = tf.transpose(self.logits, [1,0,2])\n",
    "        seq_labels = tf.transpose(self.seq_labels, [1,0])\n",
    "        seq_label_weights = tf.transpose(self.seq_label_weights, [1,0])\n",
    "        \n",
    "        # calculate the loss for each timestep for each sequence\n",
    "        raw_xent = tf.map_fn(sparse_xent, (logits, seq_labels), \n",
    "                             dtype=tf.float32)\n",
    "        weighted_xent = seq_label_weights * raw_xent\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(weighted_xent, 0)) # don't average over seq length\n",
    "    \n",
    "    def build_optimizer(self):\n",
    "        self.optimzer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimzer.minimize(self.loss, global_step=self.global_step_tensor)\n",
    "    \n",
    "    def partial_fit(self, xs, ys, evaluate_only=False):\n",
    "        xs, weights, lengths = self.preprocess_batch(xs, self.token_vocab)\n",
    "        ys, _, _ = self.preprocess_batch(ys, self.boundary_vocab)\n",
    "        feed_dict = {\n",
    "            self.token_seqs: xs,\n",
    "            self.seq_labels: ys,\n",
    "            self.seq_lens: lengths,\n",
    "            self.seq_label_weights: weights\n",
    "        }\n",
    "        if evaluate_only:\n",
    "            loss = self.session.run(self.loss, feed_dict)\n",
    "        else:\n",
    "            loss, _ = self.session.run([self.loss, self.train_op], feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        xs, _, lengths = self.preprocess_batch(xs, self.token_vocab)\n",
    "        feed_dict = {\n",
    "            self.token_seqs: xs,\n",
    "            self.seq_lens: lengths\n",
    "        }\n",
    "        preds = self.session.run(self.preds, feed_dict)\n",
    "        preds = self.postprocess_batch(preds, self.boundary_vocab, lengths=lengths)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, xs):\n",
    "        xs, _, lengths = self.preprocess_batch(xs, self.token_vocab)\n",
    "        feed_dict = {\n",
    "            self.token_seqs: xs,\n",
    "            self.seq_lens: lengths\n",
    "        }\n",
    "        probs = self.session.run(self.probs, feed_dict)\n",
    "        return probs\n",
    "    \n",
    "    def fit(self, xs, ys, n_epoch, \n",
    "            batch_size=64, validation_split=.1,\n",
    "            early_stop=True, patience=5):\n",
    "        # first carve off validation set\n",
    "        # validation_split is considered a percent if a float < 1\n",
    "        # or a hard split size if an int >= 1\n",
    "        if validation_split < 1 and type(validation_split) is float:\n",
    "            validation_split = int(validation_split*len(xs))\n",
    "        else:\n",
    "            validation_split = int(validation_split)\n",
    "        xy = zip(xs, ys)\n",
    "        npr.shuffle(xy)\n",
    "        xs, ys = zip(*xy)\n",
    "        \n",
    "        valid_xs, train_xs = xs[:validation_split], xs[validation_split:]\n",
    "        valid_ys, train_ys = ys[:validation_split], ys[validation_split:]\n",
    "        \n",
    "        # show the status\n",
    "        print \"Fitting {} epochs of {} samples per with batch size {}\".format(\n",
    "               n_epoch, len(train_xs), batch_size)\n",
    "        print \"Validation Set size is {}. \\nEarly Stop = {} with patience: {}\".format(\n",
    "               len(valid_xs), early_stop, patience)\n",
    "        \n",
    "        # do the training\n",
    "        batch_generator = BucketedBatchGenerator(train_xs, train_ys, batch_size=batch_size)\n",
    "        fit_start = time.time()\n",
    "        n_valid_loss_up = 0\n",
    "        for epoch_i in range(n_epoch):\n",
    "            epoch_start = time.time()\n",
    "            epoch_losses = []\n",
    "            for batch_i, (x, y) in enumerate(batch_generator):\n",
    "                loss = self.partial_fit(x, y)\n",
    "                \n",
    "                # logging\n",
    "                self.print_batch_loss(loss, epoch_i+1, batch_i+1, batch_generator.n_batches)\n",
    "                epoch_losses.append(loss)\n",
    "                self.append_history('batch_losses', loss)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # evaluate the validation set\n",
    "            valid_loss = self.partial_fit(valid_xs, valid_ys, evaluate_only=True)\n",
    "            self.append_history('valid_losses', valid_loss)\n",
    "            self.append_history('epoch_losses', epoch_losses)\n",
    "            self.print_epoch_loss(epoch_i+1, np.mean(epoch_losses), valid_loss, epoch_time, len(train_xs))\n",
    "            \n",
    "            # early stopping\n",
    "            if valid_loss <= self.best_valid_loss:\n",
    "                self.best_valid_loss = valid_loss\n",
    "                self.best_global_step = self.global_step\n",
    "                self.checkpoint(display=False)\n",
    "                n_valid_loss_up = 0\n",
    "            else:\n",
    "                n_valid_loss_up += 1\n",
    "                if early_stop and n_valid_loss_up > patience:\n",
    "                    print \"\\nStopping Early...\",\n",
    "                    print \"Restoring model with best valid loss = {0:2.6f} at step: {1:d}\".format(\n",
    "                        self.best_valid_loss, self.best_global_step)\n",
    "                    self.restore()\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        print \"\\nTotal train time {} sec for {} instances\".format(int(time.time()-fit_start), len(train_xs)*n_epoch)\n",
    "                \n",
    "    #####################            \n",
    "    # LOGGING FUNCTIONS #            \n",
    "    #####################            \n",
    "    def append_history(self, field_name, value):\n",
    "        \"\"\" Create or append values to a model \"history list\" by string name to keep track of important stats. \"\"\"\n",
    "        if not field_name in self.histories:\n",
    "                self.histories[field_name] = [value]\n",
    "        else:\n",
    "            self.histories[field_name].append(value)\n",
    "\n",
    "    def extend_history(self, field_name, list_of_values):\n",
    "        \"\"\" Same as `append_history` but expects an input list and extends the previous history instead of append. \"\"\"\n",
    "        if not field_name in self.histories:\n",
    "                self.histories[field_name] = list_of_values\n",
    "        else:\n",
    "            self.histories[field_name].extend(list_of_values)\n",
    "            \n",
    "    def checkpoint(self, model_name=None, display=True):\n",
    "        \"\"\" Checkpoint the model **graph** only \"\"\"\n",
    "        fname = model_name if model_name else self.model_name\n",
    "        model_file = '{}-{}.ckpt'.format(fname, self.global_step)\n",
    "        if display: print \"Saving model graph to file {} ...\".format(model_file),\n",
    "        self.saver.save(self.session, model_file)\n",
    "        self.model_file = model_file # keep track of our most recent save\n",
    "        if display: print \"Done\"\n",
    "        return model_file        \n",
    "    \n",
    "    def restore(self, model_fname=None):\n",
    "        model_fname = model_fname if model_fname else '{}-{}.ckpt'.format(self.model_name, self.best_global_step)\n",
    "        print \"Restoring model from {} ...\".format(model_fname),\n",
    "        self.saver.restore(self.session, model_fname)\n",
    "        print \"Done\"\n",
    "            \n",
    "    ####################            \n",
    "    # HELPER FUNCTIONS #\n",
    "    ####################\n",
    "    def print_batch_loss(self, loss, epoch_i, batch_i, n_batches):\n",
    "        batch_percent = float(batch_i)/n_batches\n",
    "        progress = \"Epoch {0} : [{1}{2}] {3:2.2f}%, Loss = {4:2.6f}\".format(epoch_i,\n",
    "                                                               int(np.floor(batch_percent*10))*'=',\n",
    "                                                               int(np.ceil((1-batch_percent)*10))*'-',\n",
    "                                                               batch_percent*100,\n",
    "                                                               loss)\n",
    "        if batch_i > 1:\n",
    "            print '\\r',progress,\n",
    "        else:\n",
    "            print\n",
    "            print progress,\n",
    "            \n",
    "    def print_epoch_loss(self, epoch_i, avg_loss, valid_loss, time, n_data):\n",
    "        print '\\rEpoch {0} : Avg Loss = {1:2.4f}, Validation Loss = {2:2.4f}, {3} sec for {4} instances'.format(\n",
    "            epoch_i, avg_loss, valid_loss, int(time), n_data),\n",
    "            \n",
    "    def pad_sequence(self, sequence, pad_char, pad_len):\n",
    "        return sequence + [pad_char]*(pad_len - len(sequence))\n",
    "\n",
    "    def pad_sequences(self, sequences, pad_char, pad_len=None):\n",
    "        if not pad_len:\n",
    "            pad_len = max([len(sequence) for sequence in sequences])\n",
    "        return [ self.pad_sequence(sequence, pad_char, pad_len) for sequence in sequences]\n",
    "\n",
    "    def convert_sequence(self, sequence, conversion_func):\n",
    "        return [ conversion_func(s) for s in sequence ]\n",
    "\n",
    "    def convert_sequences(self, sequences, conversion_func):\n",
    "        return [ self.convert_sequence(sequence, conversion_func) for sequence in sequences ]\n",
    "    \n",
    "    def weights_and_lengths(self, seqs, pad_char):\n",
    "        weights = np.ones_like(seqs).astype(np.float32)\n",
    "        weights[seqs == pad_char] = 0.0\n",
    "        lengths = np.sum(weights, 1).astype(np.int)\n",
    "        return weights, lengths\n",
    "    \n",
    "    def preprocess_batch(self, seqs, vocab):\n",
    "        seqs = self.pad_sequences(seqs, vocab.pad)\n",
    "        seqs = self.convert_sequences(seqs, vocab.idx)\n",
    "        seqs = np.array(seqs)\n",
    "        weights, lengths = self.weights_and_lengths(seqs, vocab.ipad)\n",
    "        return seqs, weights, lengths\n",
    "\n",
    "    def postprocess_batch(self, seqs, vocab, lengths=None):\n",
    "        seqs = seqs.tolist()\n",
    "        seqs = self.convert_sequences(seqs, vocab.token)\n",
    "        if lengths is not None:\n",
    "            for i in range(len(seqs)):\n",
    "                seqs[i] = seqs[i][:lengths[i]]\n",
    "        return seqs\n",
    "    \n",
    "    #################\n",
    "    # VISUALIZATION #\n",
    "    #################\n",
    "    def plot_learning_curve(self, \n",
    "                            logx=False, \n",
    "                            logy=False,\n",
    "                            title='Training Learning Curve',\n",
    "                            xlabel='Epoch',\n",
    "                            ylabel='Loss',\n",
    "                            figsize=(6,4),\n",
    "                            savename=None):\n",
    "        fig, ax = plt.subplots(1, figsize=figsize)\n",
    "        train_means = np.array([ np.mean(epoch_loss) for epoch_loss in self.histories['epoch_losses'] ])\n",
    "        train_stds = np.array([ np.std(epoch_loss) for epoch_loss in self.histories['epoch_losses'] ])\n",
    "        t = xrange(len(self.histories['epoch_losses']))\n",
    "        ax.plot(t, train_means, 'bo-', lw=2,label='Average Per Epoch Training Loss', markersize=1)\n",
    "        ax.fill_between(t, train_means+train_stds, train_means-train_stds, facecolor='b', alpha=0.15)\n",
    "        ax.plot(t, self.histories['valid_losses'], 'go--', lw=2, label='Per Epoch Validation Loss', markersize=1)\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        if logx:\n",
    "            ax.set_xscale('log')\n",
    "        if logy:\n",
    "            ax.set_yscale('log')\n",
    "        if savename:\n",
    "            fig.savefig(savename)\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph... Done\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "options = {\n",
    "    'token_embed_size': 50,\n",
    "    'rnn_hidden_size': 100,\n",
    "    'learning_rate':.01,\n",
    "    'token_vocab': token_vocab,\n",
    "    'boundary_vocab':boundary_vocab,\n",
    "    'checkpoint_dir':'../checkpoints'\n",
    "}\n",
    "\n",
    "model = Model(sess, 'boundary-only', options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 epochs of 337 samples per with batch size 64\n",
      "Validation Set size is 37. \n",
      "Early Stop = True with patience: 5\n",
      "\n",
      "Epoch 1 : Avg Loss = 949.0713, Validation Loss = 908.7921, 21 sec for 337 instances \n",
      "Total train time 24 sec for 337 instances\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, n_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-1eb47948e926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-258-057fb4277b33>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(self, logx, logy, title, xlabel, ylabel, figsize, savename)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mtrain_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch_losses'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mtrain_stds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Average Per Epoch Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'v' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFoCAYAAADZ17inAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAxh2xtM5qjzPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14c9d7bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_learning_curve(logy=True, savename='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_mentions(seq):\n",
    "    \"\"\" We extract mentions approximately according to the BIO or BILOU schemes\n",
    "    with some relaxations.\n",
    "    \n",
    "    We start mentions when we see anything but an 'O'. \n",
    "    We end them when we see an 'O'.\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    in_mention = False\n",
    "    mention_start = mention_end = 0\n",
    "    for i, s in enumerate(seq):\n",
    "        if not in_mention and s in ['B', 'I', 'L', 'U']:\n",
    "            mention_start = i\n",
    "            in_mention = True\n",
    "        elif in_mention and s == 'O':\n",
    "            mentions.append((mention_start, i-1))\n",
    "            in_mention=False\n",
    "    if in_mention: # we end on a mention\n",
    "        mentions.append((mention_start, i))\n",
    "    return mentions\n",
    "    \n",
    "def extract_all_mentions(seqs):\n",
    "    return [extract_mentions(seq) for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mention_precision_recall(true_mentions, pred_mentions):\n",
    "    \"\"\" This function returns the counts of true positives, false positives, and false negatives\n",
    "    which are necessary for calculating precision and recall.\n",
    "    A mention boundary is considered correct if both ends are correct. \n",
    "    \"\"\"\n",
    "    true_mentions = set(true_mentions)\n",
    "    pred_mentions = set(pred_mentions)\n",
    "    tp = len(true_mentions & pred_mentions)\n",
    "    fn = len(true_mentions - pred_mentions)\n",
    "    fp = len(pred_mentions - true_mentions)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def mention_boundary_stats(true_ys, pred_ys):\n",
    "    all_true_mentions = extract_all_mentions(true_ys)\n",
    "    all_pred_mentions = extract_all_mentions(pred_ys)\n",
    "    stats = {'tp':0,\n",
    "             'fp':0,\n",
    "             'fn':0}\n",
    "    for true_mentions, pred_mentions in zip(all_true_mentions, all_pred_mentions):\n",
    "        tp, fp, fn = mention_precision_recall(true_mentions, pred_mentions)\n",
    "        stats['tp'] += tp\n",
    "        stats['fp'] += fp\n",
    "        stats['fn'] += fn\n",
    "    stats['precision'] = tp / float(tp + fp)\n",
    "    stats['recall'] = tp / float(tp + fn)\n",
    "    stats['f1'] = 2*stats['precision']*stats['recall']/(stats['precision']+stats['recall'])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:0.3846, R: 0.4762, F1: 0.4255\n"
     ]
    }
   ],
   "source": [
    "f1_stats = mention_boundary_stats(y_test, model.predict(x_test))\n",
    "print \"P:{s[precision]:2.4f}, R: {s[recall]:2.4f}, F1: {s[f1]:2.4f}\".format(s=f1_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'<', 'O', 'O'),\n",
       " (u'DOC', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'DOCID', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'CNN_ENG_20030415_173752.0', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/DOCID', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'DOCTYPE', 'O', 'O'),\n",
       " (u'SOURCE=\"broadcast', 'O', 'O'),\n",
       " (u'news', 'O', 'O'),\n",
       " (u'\"', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'NEWS', 'O', 'O'),\n",
       " (u'STORY', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/DOCTYPE', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'DATETIME', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'2003', 'O', 'O'),\n",
       " (u'-', 'O', 'O'),\n",
       " (u'04', 'O', 'O'),\n",
       " (u'-', 'O', 'O'),\n",
       " (u'15', 'O', 'O'),\n",
       " (u'17:35:05', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/DATETIME', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'BODY', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'TEXT', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'TURN', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'spy', 'U', 'O'),\n",
       " (u'to', 'O', 'O'),\n",
       " (u'help', 'O', 'O'),\n",
       " (u'free', 'O', 'O'),\n",
       " (u'private', 'B', 'I'),\n",
       " (u'jessica', 'I', 'L'),\n",
       " (u'lynch', 'L', 'L'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'*', 'O', 'O'),\n",
       " (u'*', 'O', 'O'),\n",
       " (u'lara', 'O', 'O'),\n",
       " (u'hijacking', 'O', 'O'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'1995', 'O', 'O'),\n",
       " (u'has', 'O', 'O'),\n",
       " (u'been', 'O', 'O'),\n",
       " (u'captured', 'O', 'O'),\n",
       " (u'by', 'O', 'O'),\n",
       " (u'u.s', 'B', 'B'),\n",
       " (u'.', 'I', 'O'),\n",
       " (u'forces', 'I', 'L'),\n",
       " (u'in', 'I', 'I'),\n",
       " (u'or', 'I', 'O'),\n",
       " (u'near', 'I', 'O'),\n",
       " (u'baghdad', 'L', 'U'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'this', 'U', 'O'),\n",
       " (u'was', 'O', 'O'),\n",
       " (u'the', 'B', 'O'),\n",
       " (u'italian', 'I', 'O'),\n",
       " (u'ship', 'I', 'L'),\n",
       " (u'that', 'I', 'I'),\n",
       " (u'was', 'I', 'O'),\n",
       " (u'taken', 'I', 'O'),\n",
       " (u'--', 'I', 'O'),\n",
       " (u'that', 'I', 'O'),\n",
       " (u'was', 'I', 'O'),\n",
       " (u'captured', 'I', 'O'),\n",
       " (u'by', 'I', 'O'),\n",
       " (u'palestinian', 'I', 'B'),\n",
       " (u'terrorists', 'I', 'L'),\n",
       " (u'back', 'I', 'O'),\n",
       " (u'in', 'I', 'O'),\n",
       " (u'1985', 'L', 'O'),\n",
       " (u'and', 'O', 'O'),\n",
       " (u'some', 'U', 'B'),\n",
       " (u'may', 'O', 'O'),\n",
       " (u'remember', 'O', 'O'),\n",
       " (u'the', 'O', 'O'),\n",
       " (u'story', 'O', 'O'),\n",
       " (u'of', 'O', 'O'),\n",
       " (u'leon', 'B', 'U'),\n",
       " (u'clinghover', 'L', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u'was', 'O', 'O'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'a', 'O', 'O'),\n",
       " (u'cheal', 'O', 'O'),\n",
       " (u'chair', 'O', 'O'),\n",
       " (u'and', 'O', 'O'),\n",
       " (u'the', 'B', 'B'),\n",
       " (u'terrorists', 'L', 'L'),\n",
       " (u'shot', 'O', 'O'),\n",
       " (u'him', 'U', 'U'),\n",
       " (u'and', 'O', 'O'),\n",
       " (u'pushed', 'O', 'O'),\n",
       " (u'him', 'U', 'U'),\n",
       " (u'over', 'O', 'O'),\n",
       " (u'the', 'O', 'O'),\n",
       " (u'side', 'O', 'O'),\n",
       " (u'of', 'O', 'O'),\n",
       " (u'the', 'B', 'B'),\n",
       " (u'ship', 'L', 'L'),\n",
       " (u'into', 'O', 'O'),\n",
       " (u'the', 'B', 'B'),\n",
       " (u'mediterranean', 'L', 'O'),\n",
       " (u'where', 'O', 'I'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u'obviously', 'O', 'I'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'died', 'O', 'O'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'this', 'B', 'O'),\n",
       " (u'man', 'I', 'L'),\n",
       " (u',', 'I', 'O'),\n",
       " (u'abu', 'I', 'I'),\n",
       " (u'abbas', 'L', 'L'),\n",
       " (u'has', 'O', 'O'),\n",
       " (u'lived', 'O', 'O'),\n",
       " (u'an', 'O', 'B'),\n",
       " (u'i', 'O', 'I'),\n",
       " (u'tin', 'O', 'O'),\n",
       " (u'rant', 'O', 'O'),\n",
       " (u'life', 'O', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u\"'s\", 'O', 'O'),\n",
       " (u'been', 'O', 'O'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'tunas', 'U', 'O'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u\"'s\", 'O', 'O'),\n",
       " (u'been', 'O', 'O'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'libya', 'U', 'O'),\n",
       " (u'and', 'O', 'O'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u\"'s\", 'O', 'O'),\n",
       " (u'been', 'O', 'O'),\n",
       " (u'living', 'O', 'O'),\n",
       " (u'under', 'O', 'O'),\n",
       " (u'the', 'O', 'O'),\n",
       " (u'protection', 'O', 'O'),\n",
       " (u'of', 'O', 'O'),\n",
       " (u'saddam', 'B', 'B'),\n",
       " (u'hussein', 'L', 'L'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'baghdad', 'U', 'L'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'but', 'O', 'O'),\n",
       " (u'he', 'U', 'U'),\n",
       " (u'is', 'O', 'O'),\n",
       " (u'wanted', 'O', 'O'),\n",
       " (u'for', 'O', 'O'),\n",
       " (u'murder', 'O', 'O'),\n",
       " (u'in', 'O', 'O'),\n",
       " (u'italy', 'U', 'O'),\n",
       " (u'italy', 'U', 'O'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'there', 'O', 'O'),\n",
       " (u'are', 'O', 'O'),\n",
       " (u'charges', 'O', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'u.s', 'U', 'B'),\n",
       " (u'.', 'O', 'I'),\n",
       " (u'charges', 'O', 'O'),\n",
       " (u'which', 'O', 'O'),\n",
       " (u'have', 'O', 'O'),\n",
       " (u'expired', 'O', 'O'),\n",
       " (u'but', 'O', 'O'),\n",
       " (u'could', 'O', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'i', 'U', 'U'),\n",
       " (u'am', 'O', 'O'),\n",
       " (u'told', 'O', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'possibly', 'O', 'O'),\n",
       " (u'be', 'O', 'O'),\n",
       " (u're', 'O', 'O'),\n",
       " (u'--', 'O', 'O'),\n",
       " (u'restarted', 'O', 'O'),\n",
       " (u'for', 'O', 'O'),\n",
       " (u'piracy', 'O', 'O'),\n",
       " (u',', 'O', 'O'),\n",
       " (u'hostage', 'O', 'O'),\n",
       " (u'taking', 'O', 'O'),\n",
       " (u'and', 'O', 'O'),\n",
       " (u'conspiracy', 'O', 'O'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'so', 'O', 'O'),\n",
       " (u'this', 'O', 'O'),\n",
       " (u'is', 'O', 'O'),\n",
       " (u'one', 'O', 'O'),\n",
       " (u'of', 'O', 'O'),\n",
       " (u'the', 'O', 'O'),\n",
       " (u'oldest', 'O', 'I'),\n",
       " (u'cases', 'O', 'O'),\n",
       " (u'of', 'O', 'O'),\n",
       " (u'terrorism', 'O', 'O'),\n",
       " (u'that', 'O', 'O'),\n",
       " (u'is', 'O', 'O'),\n",
       " (u'one', 'O', 'B'),\n",
       " (u'of', 'O', 'I'),\n",
       " (u'the', 'O', 'I'),\n",
       " (u'oldest', 'O', 'I'),\n",
       " (u'cases', 'O', 'I'),\n",
       " (u'that', 'O', 'O'),\n",
       " (u'has', 'O', 'O'),\n",
       " (u'not', 'O', 'O'),\n",
       " (u'been', 'O', 'O'),\n",
       " (u'closed', 'O', 'O'),\n",
       " (u'to', 'O', 'O'),\n",
       " (u'date', 'O', 'O'),\n",
       " (u't.', 'O', 'O'),\n",
       " (u'now', 'O', 'O'),\n",
       " (u'look', 'O', 'O'),\n",
       " (u'as', 'O', 'O'),\n",
       " (u'if', 'O', 'O'),\n",
       " (u'this', 'B', 'B'),\n",
       " (u'man', 'I', 'I'),\n",
       " (u'abu', 'I', 'I'),\n",
       " (u'absaas', 'L', 'I'),\n",
       " (u'in', 'O', 'I'),\n",
       " (u'u.s', 'B', 'I'),\n",
       " (u'.', 'I', 'I'),\n",
       " (u'hands', 'L', 'O'),\n",
       " (u'.', 'O', 'O'),\n",
       " (u'miles', 'U', 'O'),\n",
       " (u'?', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/TURN', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/TEXT', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/BODY', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'ENDTIME', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'2003', 'O', 'O'),\n",
       " (u'-', 'O', 'O'),\n",
       " (u'04', 'O', 'O'),\n",
       " (u'-', 'O', 'O'),\n",
       " (u'15', 'O', 'O'),\n",
       " (u'17:36:17', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/ENDTIME', 'O', 'O'),\n",
       " (u'>', 'O', 'O'),\n",
       " (u'<', 'O', 'O'),\n",
       " (u'/DOC', 'O', 'O'),\n",
       " (u'>', 'O', 'O')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(x_test[0], y_test[0], model.predict(x_test[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
