{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import json\n",
    "import io\n",
    "import time\n",
    "import os.path\n",
    "import six\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set_color_codes()\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "\n",
    "import chainer as ch\n",
    "import chainer.training.extensions # for some reason this isn't automatically imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# allow for importing local infonet package\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# import sys\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from infonet.vocab import Vocab\n",
    "from infonet.preprocess import compute_flat_mention_labels, Entity_BIO_map\n",
    "from infonet.util import convert_sequences, SequenceIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.loads(io.open('../data/ace_05_head_yaat.json', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get vocabs\n",
    "token_vocab = Vocab(min_count=5)\n",
    "for doc in data.values():\n",
    "    token_vocab.add(doc['tokens'])\n",
    "\n",
    "boundary_vocab = Vocab(min_count=0)\n",
    "for doc in data.values():\n",
    "    doc['boundary_labels'] = compute_flat_mention_labels(doc, Entity_BIO_map)\n",
    "    boundary_vocab.add(doc['boundary_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 28, u'value:TIME'), (62, 67, u'value:TIME'), (156, 158, u'value:Numeric'), (181, 183, u'value:Numeric'), (230, 231, u'entity:GPE'), (240, 241, u'entity:GPE'), (250, 251, u'entity:GPE'), (291, 292, u'entity:ORG'), (381, 382, u'entity:ORG'), (293, 294, u'entity:GPE'), (346, 347, u'entity:GPE'), (361, 362, u'entity:PER'), (379, 380, u'entity:GPE'), (75, 76, u'entity:ORG'), (92, 93, u'entity:ORG'), (55, 56, u'entity:PER'), (90, 91, u'entity:PER'), (374, 375, u'entity:PER'), (131, 132, u'entity:PER'), (347, 348, u'entity:PER'), (136, 137, u'entity:GPE'), (206, 207, u'entity:GPE'), (212, 213, u'entity:GPE'), (225, 226, u'entity:GPE'), (270, 271, u'entity:GPE'), (306, 307, u'entity:GPE'), (335, 336, u'entity:GPE'), (354, 355, u'entity:GPE'), (150, 151, u'entity:GPE'), (151, 152, u'entity:PER'), (153, 154, u'entity:PER'), (332, 333, u'entity:PER'), (163, 164, u'entity:PER'), (173, 174, u'entity:GPE'), (208, 209, u'entity:GPE')]\n"
     ]
    }
   ],
   "source": [
    "# print data.values()[0]['annotations']\n",
    "def compute_mentions(doc, fine_grained=False):\n",
    "    mentions = []\n",
    "    for ann in doc['annotations']:\n",
    "        if ann['ann-type'] == u'node':\n",
    "            mention_type = ann['node-type']+':'+ann['type']\n",
    "            if fine_grained:\n",
    "                mention_type += ':'+ann['subtype']\n",
    "            mentions.append((ann['ann-span'][0], ann['ann-span'][1], mention_type))\n",
    "    return mentions\n",
    "print compute_mentions(data.values()[0])\n",
    "\n",
    "def compute_relations(doc, fine_grained=False):\n",
    "    relations = []\n",
    "    id2ann = { ann['ann-uid']:ann for ann in doc['annotations']}\n",
    "    for ann in doc['annotations']:\n",
    "        if ann['ann-type'] == u'edge':\n",
    "            left_span = id2ann[ann['ann-left']]['ann-span']\n",
    "            right_span = id2ann[ann['ann-right']]['ann-span']\n",
    "            rel_type = ann['edge-type']+':'+ann['type']\n",
    "            if fine_grained:\n",
    "                rel_type += ':'+ann['subtype']\n",
    "            relations.append((left_span[0], left_span[1], right_span[0], right_span[1], rel_type))\n",
    "    return relations\n",
    "# print compute_relations(data.values()[0])\n",
    "\n",
    "mention_vocab = Vocab(min_count=0)\n",
    "relation_vocab = Vocab(min_count=0)\n",
    "for doc in data.values():\n",
    "    doc['mentions'] = compute_mentions(doc)\n",
    "    mention_vocab.add([ m[2] for m in doc['mentions'] ])\n",
    "    doc['relations'] = compute_relations(doc)\n",
    "    relation_vocab.add([r[4] for r in doc['relations']])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 53713\n",
      "set([u'entity:WEA', u'value:Job-Title', u'entity:FAC', u'value:Crime', u'entity:PER', u'entity:VEH', u'value:Sentence', u'entity:LOC', u'value:Numeric', u'event-anchor:Conflict', u'event-anchor:Contact', '<UNK>', u'event-anchor:Life', u'value:TIME', '<PAD>', u'value:Contact-Info', u'event-anchor:Personnel', u'event-anchor:Movement', u'event-anchor:Business', u'entity:ORG', u'entity:GPE', u'event-anchor:Transaction', u'event-anchor:Justice'])\n",
      "--------------------------------------------------\n",
      "71 235619\n",
      "set([u'relation:--PHYS->', u'event-argument:<-ARG:Beneficiary--', u'event-argument:--ARG:Recipient->', u'event-argument:<-ARG:Person--', u'relation:--PER-SOC->', u'event-argument:--ARG:Sentence->', u'event-argument:--ARG:Entity->', u'relation:<-PHYS--', u'event-argument:--ARG:Destination->', u'event-argument:--ARG:Artifact->', u'event-argument:--ARG:Position->', u'event-argument:--ARG:Person->', u'event-argument:--ARG:Plaintiff->', u'event-argument:<-ARG:Entity--', u'coreference:--SameAs--', u'relation:--PART-WHOLE->', u'event-argument:<-ARG:Victim--', u'event-argument:<-ARG:Target--', u'event-argument:--ARG:Defendant->', u'relation:<-ART--', u'event-argument:<-ARG:Vehicle--', u'event-argument:--ARG:Price->', u'event-argument:<-ARG:Org--', u'event-argument:<-ARG:Defendant--', u'event-argument:--ARG:Money->', u'relation:--ORG-AFF->', u'relation:<-GEN-AFF--', u'relation:--GEN-AFF->', u'relation:--ART->', u'event-argument:<-ARG:Destination--', u'event-argument:<-ARG:Place--', '<PAD>', u'event-argument:<-ARG:Adjudicator--', u'relation:<-PER-SOC--', u'event-argument:--ARG:Victim->', u'event-argument:<-ARG:Position--', u'event-argument:<-ARG:Recipient--', u'event-argument:--ARG:Target->', '<UNK>', u'event-argument:<-ARG:Origin--', u'event-argument:<-ARG:Giver--', u'event-argument:<-ARG:Buyer--', u'event-argument:<-ARG:Agent--', u'event-argument:--ARG:Seller->', u'event-argument:<-ARG:Money--', u'event-argument:<-ARG:Attacker--', u'event-argument:--ARG:Buyer->', u'event-argument:<-ARG:Sentence--', u'event-argument:--ARG:Attacker->', u'event-argument:--ARG:Place->', u'event-argument:<-ARG:Crime--', u'event-argument:--ARG:Prosecutor->', u'event-argument:<-ARG:Instrument--', u'event-argument:--ARG:Origin->', u'event-argument:--ARG:Org->', u'event-argument:--ARG:Agent->', u'event-argument:<-ARG:Artifact--', u'event-argument:<-ARG:Seller--', u'event-argument:<-ARG:Plaintiff--', u'event-argument:<-ARG:Price--', u'event-argument:<-ARG:Time--', u'event-argument:--ARG:Crime->', u'relation:<-ORG-AFF--', u'event-argument:--ARG:Adjudicator->', u'event-argument:--ARG:Giver->', u'relation:<-PART-WHOLE--', u'event-argument:<-ARG:Prosecutor--', u'event-argument:--ARG:Vehicle->', u'event-argument:--ARG:Beneficiary->', u'event-argument:--ARG:Instrument->', u'event-argument:--ARG:Time->'])\n"
     ]
    }
   ],
   "source": [
    "print mention_vocab.v, mention_vocab.n\n",
    "print mention_vocab.vocabset\n",
    "print '-'*50\n",
    "print relation_vocab.v, relation_vocab.n\n",
    "print relation_vocab.vocabset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 train, 53 validation, and 54 test documents\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "test = .1\n",
    "valid = .1\n",
    "dataset = [(doc['tokens'], doc['boundary_labels'], doc['mentions'], doc['relations']) for doc in data.values()]\n",
    "npr.shuffle(dataset)\n",
    "test = 1-test # eg, .1 -> .9\n",
    "valid = test-valid # eg, .1 -> .9\n",
    "valid_split = int(len(dataset)*valid)\n",
    "test_split = int(len(dataset)*test)\n",
    "dataset_train, dataset_valid, dataset_test = (dataset[:valid_split], \n",
    "                                              dataset[valid_split:test_split], \n",
    "                                              dataset[test_split:])\n",
    "\n",
    "x_train = [d[0] for d in dataset_train]\n",
    "b_train = [d[1] for d in dataset_train]\n",
    "m_train = [d[2] for d in dataset_train]\n",
    "r_train = [d[3] for d in dataset_train]\n",
    "\n",
    "x_valid = [d[0] for d in dataset_valid]\n",
    "b_valid = [d[1] for d in dataset_valid]\n",
    "m_valid = [d[2] for d in dataset_valid]\n",
    "r_valid = [d[3] for d in dataset_valid]\n",
    "\n",
    "x_test = [d[0] for d in dataset_test]\n",
    "b_test = [d[1] for d in dataset_test]\n",
    "m_test = [d[2] for d in dataset_test]\n",
    "r_test = [d[3] for d in dataset_test]\n",
    "\n",
    "print '{} train, {} validation, and {} test documents'.format(len(x_train), len(x_valid), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 'a')\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print (1,2)+tuple('a')\n",
    "print (0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up... Done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# convert dataset to idxs\n",
    "# before we do conversions, we need to drop unfrequent words from the vocab and reindex it\n",
    "print \"Setting up...\",\n",
    "token_vocab.drop_infrequent()\n",
    "boundary_vocab.drop_infrequent()\n",
    "mention_vocab.drop_infrequent()\n",
    "relation_vocab.drop_infrequent()\n",
    "\n",
    "ix_train = convert_sequences(x_train, token_vocab.idx)\n",
    "ix_valid = convert_sequences(x_valid, token_vocab.idx)\n",
    "ix_test = convert_sequences(x_test, token_vocab.idx)\n",
    "ib_train = convert_sequences(b_train, boundary_vocab.idx)\n",
    "ib_valid = convert_sequences(b_valid, boundary_vocab.idx)\n",
    "ib_test = convert_sequences(b_test, boundary_vocab.idx)\n",
    "convert_mention = lambda x:x[:-1]+(mention_vocab.idx(x[-1]),)\n",
    "im_train = convert_sequences(m_train, convert_mention)\n",
    "im_valid = convert_sequences(m_valid, convert_mention)\n",
    "im_test = convert_sequences(m_test, convert_mention)\n",
    "convert_relation = lambda x:x[:-1]+(relation_vocab.idx(x[-1]),)\n",
    "ir_train = convert_sequences(r_train, convert_relation)\n",
    "ir_valid = convert_sequences(r_valid, convert_relation)\n",
    "ir_test = convert_sequences(r_test, convert_relation)\n",
    "\n",
    "# data\n",
    "train_iter = SequenceIterator(zip(ix_train, ib_train, im_train, ir_train), batch_size, repeat=True)\n",
    "valid_iter = SequenceIterator(zip(ix_valid, ib_valid, im_valid, ir_valid), batch_size, repeat=True)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from infonet.tagger import Tagger, extract_all_mentions\n",
    "class Extractor(ch.Chain):\n",
    "    def __init__(self, \n",
    "                 feature_size,\n",
    "                 n_mention_class,\n",
    "                 n_relation_class,\n",
    "                 in_tags=(1,2), \n",
    "                 out_tags=(0,),\n",
    "                 max_rel_dist=1000):\n",
    "        super(Extractor, self).__init__(\n",
    "            f_m=ch.links.Linear(feature_size, n_mention_class),\n",
    "            f_r=ch.links.Linear(2*feature_size, n_relation_class)\n",
    "        )\n",
    "#         self.f_m.W.data = npr.randint((feature_size, n_mention_class))\n",
    "        self.in_tags = in_tags\n",
    "        self.out_tags = out_tags\n",
    "        self.max_rel_dist = max_rel_dist\n",
    "        \n",
    "    def _extract_graph(self, tagger_preds, tagger_features):\n",
    "        # convert from time-major to batch-major\n",
    "        tagger_preds = ch.functions.transpose_sequence(tagger_preds)\n",
    "        tagger_features = ch.functions.transpose_sequence(tagger_features)\n",
    "#         print len(tagger_preds)\n",
    "        \n",
    "        # extract the mentions and relations for each doc\n",
    "        all_boundaries = extract_all_mentions(tagger_preds, \n",
    "                                              in_tags=self.in_tags, \n",
    "                                              out_tags=self.out_tags)\n",
    "#         print 'N', [len(b) for b in all_boundaries]\n",
    "        all_mentions = []\n",
    "        all_relation_idxs = []\n",
    "        all_left_mentions = []\n",
    "        all_right_mentions = []\n",
    "        for s, (boundaries, seq, features) in enumerate(zip(all_boundaries, tagger_preds, tagger_features)):\n",
    "            mentions = []\n",
    "            relation_idxs = []\n",
    "            left_mentions = []\n",
    "            right_mentions = []\n",
    "            for i, b in enumerate(boundaries):\n",
    "                mention = ch.functions.sum(features[b[0]:b[1]], axis=0)\n",
    "                mentions.append(mention)\n",
    "                # make a relation of to all previous mentions (M choose 2)\n",
    "                for j in range(i):\n",
    "                    if abs(boundaries[j][0] - b[0]) < self.max_rel_dist:\n",
    "                        print '\\rDoc: {} R({},{})'.format(s, j, i),\n",
    "                        relation_idxs.append([j,i])\n",
    "                        left_mentions.append(mentions[j])\n",
    "                        right_mentions.append(mentions[i])\n",
    "            print 'Stacking...',\n",
    "            if mentions:\n",
    "                mentions = ch.functions.vstack(mentions)\n",
    "                print 'm=',mentions.shape,\n",
    "            all_mentions.append(mentions)\n",
    "            if left_mentions:\n",
    "                left_mentions = ch.functions.vstack(left_mentions)\n",
    "                print 'l=',left_mentions.shape,\n",
    "            all_left_mentions.append(left_mentions)\n",
    "            if right_mentions:\n",
    "                right_mentions = ch.functions.vstack(right_mentions)\n",
    "                print 'r=',right_mentions.shape,\n",
    "            all_right_mentions.append(right_mentions)\n",
    "            all_relation_idxs.append(relation_idxs)\n",
    "            print 'Done'\n",
    "        return all_mentions, all_left_mentions, all_right_mentions, all_relation_idxs\n",
    "    \n",
    "    def __call__(self, tagger_logits, tagger_features):\n",
    "        tagger_preds = [ ch.functions.argmax(logit, axis=1) for logit in tagger_logits ]\n",
    "        start = time.time()\n",
    "        mentions, l_mentions, r_mentions, rel_idxs = self._extract_graph(    \n",
    "            tagger_preds, \n",
    "            tagger_features)\n",
    "        # concat left and right mentions into one relation vector\n",
    "        relations = [ ch.functions.concat(m, axis=1) if type(m[0]) is ch.Variable else m[0]\n",
    "                             for m in zip(l_mentions, r_mentions) ]\n",
    "        # score mentions and relations\n",
    "#         print mentions[0].shape\n",
    "        m_logits = [ self.f_m(m) if type(m) is ch.Variable else []\n",
    "                     for m in mentions ]\n",
    "        r_logits = [ self.f_r(r) if type(r) is ch.Variable else []\n",
    "                     for r in relations ]\n",
    "        return m_logits, r_logits, rel_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
